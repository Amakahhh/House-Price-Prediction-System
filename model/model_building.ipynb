{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd19904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model and verify\n",
    "print(\"Loading saved model...\")\n",
    "loaded_model = joblib.load(model_path)\n",
    "loaded_scaler = joblib.load(scaler_path)\n",
    "loaded_features = joblib.load(feature_names_path)\n",
    "\n",
    "print(\"Model reloaded successfully!\")\n",
    "\n",
    "# Test on sample data\n",
    "sample_test_pred_original = model.predict(X_test_scaled[:5])\n",
    "sample_test_pred_loaded = loaded_model.predict(loaded_scaler.transform(X_test_encoded[:5]))\n",
    "\n",
    "# Compare predictions\n",
    "print(\"\\nVerification - Comparing original vs reloaded model predictions:\")\n",
    "print(\"=\"*60)\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i+1}: Original=${sample_test_pred_original[i]:,.2f} | Loaded=${sample_test_pred_loaded[i]:,.2f}\")\n",
    "\n",
    "# Check if predictions are identical\n",
    "if np.allclose(sample_test_pred_original, sample_test_pred_loaded):\n",
    "    print(\"\\n✓ Model reloaded successfully! Predictions are identical.\")\n",
    "else:\n",
    "    print(\"\\n✗ Warning: Predictions differ slightly.\")\n",
    "\n",
    "print(\"\\nModel is ready for deployment!\")\n",
    "print(f\"Features used: {loaded_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6db484",
   "metadata": {},
   "source": [
    "## Section 10: Verify Model Reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create model directory if it doesn't exist\n",
    "model_dir = '../model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "model_path = os.path.join(model_dir, 'house_price_model.pkl')\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save the scaler\n",
    "scaler_path = os.path.join(model_dir, 'scaler.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save feature names for later use\n",
    "feature_names_path = os.path.join(model_dir, 'feature_names.pkl')\n",
    "joblib.dump(X_train_encoded.columns.tolist(), feature_names_path)\n",
    "print(f\"Feature names saved to: {feature_names_path}\")\n",
    "\n",
    "print(\"\\nModel artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8d3ff",
   "metadata": {},
   "source": [
    "## Section 9: Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics for training set\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate evaluation metrics for test set\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTRAINING SET METRICS:\")\n",
    "print(f\"  Mean Absolute Error (MAE):        ${train_mae:,.2f}\")\n",
    "print(f\"  Mean Squared Error (MSE):         ${train_mse:,.2f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE):   ${train_rmse:,.2f}\")\n",
    "print(f\"  R² Score:                         {train_r2:.4f}\")\n",
    "\n",
    "print(\"\\nTEST SET METRICS:\")\n",
    "print(f\"  Mean Absolute Error (MAE):        ${test_mae:,.2f}\")\n",
    "print(f\"  Mean Squared Error (MSE):         ${test_mse:,.2f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE):   ${test_rmse:,.2f}\")\n",
    "print(f\"  R² Score:                         {test_r2:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Model explains {test_r2*100:.2f}% of variance in test data\")\n",
    "print(f\"Average prediction error: ${test_mae:,.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2efd38",
   "metadata": {},
   "source": [
    "## Section 8: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893b076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Random Forest Regressor model\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest Regressor Model...\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"Model training completed!\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "feature_names = X_train_encoded.columns.tolist()\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346e1f4",
   "metadata": {},
   "source": [
    "## Section 6 & 7: Implement and Train the Model\n",
    "Using Random Forest Regressor for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6cf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler for feature normalization\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler on training data only\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "print(\"Feature Scaling Applied:\")\n",
    "print(f\"Training set mean: {X_train_scaled.mean(axis=0)[:5]}...\")\n",
    "print(f\"Training set std: {X_train_scaled.std(axis=0)[:5]}...\")\n",
    "print(f\"\\nScaling complete. Features normalized to have mean=0 and std=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d00109f",
   "metadata": {},
   "source": [
    "## Section 5: Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e637ba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical variables\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Use One-Hot Encoding for categorical variables\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Ensure both datasets have the same columns\n",
    "X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "\n",
    "print(f\"\\nShape after encoding:\")\n",
    "print(f\"Training set: {X_train_encoded.shape}\")\n",
    "print(f\"Testing set: {X_test_encoded.shape}\")\n",
    "print(f\"\\nFeature columns: {X_train_encoded.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8981d2",
   "metadata": {},
   "source": [
    "## Section 4: Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68993bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 6 features from the recommended 9\n",
    "# Selected features: OverallQual, GrLivArea, TotalBsmtSF, GarageCars, YearBuilt, Neighborhood\n",
    "selected_features = ['OverallQual', 'GrLivArea', 'TotalBsmtSF', 'GarageCars', 'YearBuilt', 'Neighborhood']\n",
    "target = 'SalePrice'\n",
    "\n",
    "print(\"Selected Features for Model:\")\n",
    "print(selected_features)\n",
    "print(\"\\nRationale: These features have strong correlation with house prices:\")\n",
    "print(\"- OverallQual: Overall material and finish quality\")\n",
    "print(\"- GrLivArea: Above grade (ground) living area\")\n",
    "print(\"- TotalBsmtSF: Total basement area\")\n",
    "print(\"- GarageCars: Size of garage in car capacity\")\n",
    "print(\"- YearBuilt: Original construction year\")\n",
    "print(\"- Neighborhood: Physical location of the property\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = train_data[selected_features].copy()\n",
    "y = train_data[target].copy()\n",
    "\n",
    "# Split into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f70d03f",
   "metadata": {},
   "source": [
    "## Section 3: Feature Selection and Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77a6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in recommended features\n",
    "print(\"Missing Values in Recommended Features:\")\n",
    "print(train_data[recommended_features].isnull().sum())\n",
    "\n",
    "# Handle missing values\n",
    "# Strategy: Mean imputation for numerical, Mode for categorical\n",
    "for col in recommended_features:\n",
    "    if train_data[col].isnull().sum() > 0:\n",
    "        if train_data[col].dtype == 'object':\n",
    "            train_data[col].fillna(train_data[col].mode()[0], inplace=True)\n",
    "            print(f\"Filled missing values in {col} with mode\")\n",
    "        else:\n",
    "            train_data[col].fillna(train_data[col].mean(), inplace=True)\n",
    "            print(f\"Filled missing values in {col} with mean\")\n",
    "\n",
    "print(\"\\nMissing values after handling:\")\n",
    "print(train_data[recommended_features].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92c0f9",
   "metadata": {},
   "source": [
    "## Section 2: Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv('https://raw.githubusercontent.com/awesomedata/awesome-public-datasets/master/datasets/Kaggle/train.csv')\n",
    "\n",
    "print(\"Dataset Shape:\", train_data.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(train_data.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(train_data.info())\n",
    "print(\"\\nRecommended Features:\")\n",
    "recommended_features = ['OverallQual', 'GrLivArea', 'TotalBsmtSF', 'GarageCars', 'BedroomAbvGr', 'FullBath', 'YearBuilt', 'Neighborhood', 'SalePrice']\n",
    "print(train_data[recommended_features].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4cd51f",
   "metadata": {},
   "source": [
    "## Section 1: Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f90f5",
   "metadata": {},
   "source": [
    "# House Price Prediction System - Model Development\n",
    "This notebook implements a machine learning model to predict house prices using the House Prices: Advanced Regression Techniques dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
